# # ingestion/indexer.py
# from ingestion.loader import scan_documents
# from ingestion.parser import parse_pdf
# from ingestion.chunker import chunk_blocks
# from embedding.embedder import Embedder
# from storage.milvus_store import MilvusVectorStore, Chunk
# from config.settings import settings
# from tqdm import tqdm

# def build_index(source_dir="sourcepdf"):
#     """
#     æ„å»ºä¿é™©çŸ¥è¯†åº“ç´¢å¼•ï¼š
#     1. æ‰«ææ‰€æœ‰æ–‡ä»¶
#     2. æŠ½å–æ–‡å­— + è¡¨æ ¼ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
#     3. å¯¹æ–‡å­—å†…å®¹åˆ†å—
#     4. åµŒå…¥ + å†™å…¥ Milvus
#     """
#     print("ğŸš€ å¼€å§‹æ„å»ºç´¢å¼• ...")
#     docs = scan_documents(source_dir)
#     if not docs:
#         print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯ç´¢å¼•çš„æ–‡ä»¶ã€‚")
#         return

#     embedder = Embedder()
#     store = MilvusVectorStore()
#     total_chunks = 0

#     for doc in tqdm(docs, desc="ç´¢å¼•è¿›åº¦"):
#         try:
#             parsed_blocks = parse_pdf(doc["path"])
#             # print(f"ğŸ“„ è§£æå®Œæˆï¼š{doc['path']}ï¼Œæå–åˆ° {len(parsed_blocks)} ä¸ªå†…å®¹å—ã€‚")
#             # print(f"é¢„è§ˆå†…å®¹å—ï¼š{parsed_blocks[:2]}")  # æ‰“å°å‰ä¸¤ä¸ªå†…å®¹å—ä»¥ä¾›è°ƒè¯•
#             if not parsed_blocks:
#                 print(f"âš ï¸ æ–‡ä»¶æ— æœ‰æ•ˆå†…å®¹ï¼š{doc['path']}")
#                 continue

#             for block in parsed_blocks:
#                 # content = block.get("text", "").strip()
#                 # modality = block.get("modality", "text")
#                 # page = block.get("page", 0)
                

#                 # è·³è¿‡ç©ºå—
#                 if not content:
#                     continue

#                 # ä»…æ–‡æœ¬è¿›è¡Œåˆ†å—ï¼›è¡¨æ ¼ä¿æŒæ•´å—
#                 if modality == "text":
#                     chunks = chunk_blocks(parsed_blocks)
#                 else:
#                     chunks = [content]
                
#                 for c in chunks:
#                     emb = embedder.embed_text([c])[0]
#                     meta = {
#                         **doc["metadata"],
#                         "page": page,
#                         "modality": modality
#                     }
#                     store.add([emb], [Chunk(c, meta)])
#                     total_chunks += 1

#         except Exception as e:
#             print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {doc['path']} ({e})")

#     print(f"âœ… ç´¢å¼•å®Œæˆï¼Œå…±å†™å…¥ {total_chunks} ä¸ªæ–‡æœ¬å—ã€‚")

from ingestion.loader import scan_documents
from ingestion.parser import parse_pdf
from ingestion.chunker import chunk_blocks
from embedding.embedder import Embedder
from storage.milvus_store import MilvusVectorStore, Chunk
from tqdm import tqdm
import time

def build_index(source_dir="sourcepdf"):
    """
    æ„å»ºä¿é™©çŸ¥è¯†åº“ç´¢å¼•ï¼š
    1. æ‰«ææ‰€æœ‰æ–‡ä»¶
    2. æŠ½å–æ–‡å­— + è¡¨æ ¼ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    3. å¯¹æ–‡å­—å†…å®¹åˆ†å—
    4. åµŒå…¥ + å†™å…¥ Milvus
    """
    print("ğŸš€ å¼€å§‹æ„å»ºç´¢å¼• ...")
    docs = scan_documents(source_dir)
    if not docs:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯ç´¢å¼•çš„æ–‡ä»¶ã€‚")
        return

    embedder = Embedder()
    store = MilvusVectorStore()
    total_chunks = 0
    batch_chunks = []
    batch_texts = []
    batch_embs = []
    batch_size = 500  # æ¯æ‰¹å¤„ç†çš„æ–‡æœ¬å—æ•°é‡

    for doc in tqdm(docs, desc="ç´¢å¼•è¿›åº¦"):
        try:
            parsed_blocks = parse_pdf(doc["path"])
            if not parsed_blocks:
                print(f"âš ï¸ æ–‡ä»¶æ— æœ‰æ•ˆå†…å®¹ï¼š{doc['path']}")
                continue

            # ç»™æ¯ä¸ª block æ·»åŠ  metadata
            for b in parsed_blocks:
                b.setdefault("metadata", {})
                b["metadata"].update({
                    "source": doc.get("path", ""),
                    "company": doc.get("company", ""),
                    "category": doc.get("category", ""),
                    "page_number": b.get("page_number", None),
                    "modality": b.get("modality", "text")
                })

            chunks = chunk_blocks(parsed_blocks, max_len=500, overlap=50)

            # âœ… åµŒå…¥å¹¶å†™å…¥ Milvus
            for c in chunks:
                text = c.get("text", "").strip()
                if not text:
                    continue
                meta = c.get("metadata", {})
                # emb = embedder.embed_text([text])[0]
                # store.add([emb], [Chunk(text, meta)])
                # total_chunks += 1
                batch_chunks.append(Chunk(text, meta))
                batch_texts.append(text)

                if len(batch_chunks) >= batch_size:
                    _flush_batch(store, embedder, batch_chunks, batch_texts)
                    total_chunks += len(batch_chunks)
                    batch_chunks = []
                    batch_texts = []
        except Exception as e:
            print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {doc['path']} ({e})")
    # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
    if batch_chunks:
        _flush_batch(store, embedder, batch_chunks, batch_texts)
        total_chunks += len(batch_chunks)

    print(f"âœ… ç´¢å¼•å®Œæˆï¼Œå…±å†™å…¥ {total_chunks} ä¸ªæ–‡æœ¬å—ã€‚")

def _flush_batch(store, embedder, batch_chunks, batch_texts):
    try:
        start = time.time()
        embeddings = embedder.embed_text(batch_texts)
        store.add(embeddings, batch_chunks)
        cost = time.time() - start
        print(f"[æ‰¹æ¬¡å†™å…¥] âœ… å†™å…¥ {len(batch_chunks)} ä¸ªæ–‡æœ¬å—ï¼Œè€—æ—¶ {cost:.2f} ç§’ã€‚")
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡å†™å…¥å¤±è´¥: {e}")  